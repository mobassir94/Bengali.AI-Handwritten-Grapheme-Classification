{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cutmix_b4_.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB63EN0Xqpur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e47435c-9c38-4d5b-b022-484b9de14ee2"
      },
      "source": [
        "print('hi')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1tZo5szaslQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c5ed4eab-787c-491b-f623-1e3b2ae5604d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs1Sjzb9bJ_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "outputId": "61723804-a939-4443-e9d3-ffb0e6c33591"
      },
      "source": [
        "!pip install albumentations==0.4.3"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting albumentations==0.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/c4/a1e6ac237b5a27874b01900987d902fe83cc469ebdb09eb72a68c4329e78/albumentations-0.4.3.tar.gz (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.3) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.3) (1.4.1)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.3) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.3) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (6.2.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (2.4)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (3.1.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (1.1.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (4.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.3) (45.2.0)\n",
            "Building wheels for collected packages: albumentations, imgaug\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.3-cp36-none-any.whl size=60764 sha256=670f1d32598c79eeb69645c0ee00c5cdefd08cf6df441715c770f6e8ddb85b42\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/16/8e/d3bec34bf30adff30929226f0b83cc8c005b5af131f51db9d0\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=b7b8f775dac7b0ae3b8401706eae55ab1289751956936c79278fb9f295a4f717\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built albumentations imgaug\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.3 imgaug-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kPSwISobRVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division, absolute_import\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchvision import transforms,models\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
        "\n",
        "from albumentations import (\n",
        "    PadIfNeeded,\n",
        "    GaussianBlur,\n",
        "    Flip,\n",
        "    HorizontalFlip,\n",
        "    Transpose,\n",
        "    VerticalFlip,    \n",
        "    CenterCrop,    \n",
        "    Crop,\n",
        "    Compose,\n",
        "    Transpose,\n",
        "    RandomRotate90,\n",
        "    ElasticTransform,\n",
        "    GridDistortion, \n",
        "    OpticalDistortion,\n",
        "    RandomSizedCrop,\n",
        "    OneOf,\n",
        "    CLAHE,\n",
        "    Rotate,\n",
        "    RandomBrightnessContrast,    \n",
        "    RandomGamma,\n",
        "    ShiftScaleRotate ,\n",
        "    GaussNoise,\n",
        "    Blur,\n",
        "    MotionBlur,IAAAdditiveGaussianNoise,Cutout,IAAAffine\n",
        ")\n",
        "my_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2TqIEwFbRi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_aug = Compose([ \n",
        "    ShiftScaleRotate(rotate_limit=10, scale_limit=.1),Cutout(num_holes=1, max_h_size=64, max_w_size=64, fill_value=0, always_apply=False, p=0.5),\n",
        "    OneOf([\n",
        "        GridDistortion(distort_limit =0.05 ,border_mode=cv2.BORDER_CONSTANT,value =1, p=0.1),\n",
        "        OpticalDistortion(p=0.1, distort_limit= 0.05, shift_limit=0.2,border_mode=cv2.BORDER_CONSTANT,value =1)                  \n",
        "        ], p=0.5),\n",
        "    OneOf([\n",
        "        Blur(),\n",
        "        GaussianBlur(blur_limit=1),IAAAdditiveGaussianNoise(p = 0.3),IAAAffine(shear=20, mode='constant'),\n",
        "        ], p=0.5),    \n",
        "    RandomGamma(p=0.3)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XH5debhb0do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x): \n",
        "        \n",
        "        x = x *( torch.tanh(F.softplus(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "import re\n",
        "import math\n",
        "import collections\n",
        "from functools import partial\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import model_zoo\n",
        "\n",
        "########################################################################\n",
        "############### HELPERS FUNCTIONS FOR MODEL ARCHITECTURE ###############\n",
        "########################################################################\n",
        "\n",
        "\n",
        "# Parameters for the entire model (stem, all blocks, and head)\n",
        "GlobalParams = collections.namedtuple('GlobalParams', [\n",
        "    'batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate',\n",
        "    'num_classes', 'width_coefficient', 'depth_coefficient',\n",
        "    'depth_divisor', 'min_depth', 'drop_connect_rate', 'image_size'])\n",
        "\n",
        "# Parameters for an individual model block\n",
        "BlockArgs = collections.namedtuple('BlockArgs', [\n",
        "    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n",
        "    'expand_ratio', 'id_skip', 'stride', 'se_ratio'])\n",
        "\n",
        "# Change namedtuple defaults\n",
        "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
        "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n",
        "\n",
        "\n",
        "class SwishImplementation(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, i):\n",
        "        result = i * torch.sigmoid(i)\n",
        "        ctx.save_for_backward(i)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        i = ctx.saved_variables[0]\n",
        "        sigmoid_i = torch.sigmoid(i)\n",
        "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
        "\n",
        "\n",
        "class MemoryEfficientSwish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return SwishImplementation.apply(x)\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def round_filters(filters, global_params):\n",
        "    \"\"\" Calculate and round number of filters based on depth multiplier. \"\"\"\n",
        "    multiplier = global_params.width_coefficient\n",
        "    if not multiplier:\n",
        "        return filters\n",
        "    divisor = global_params.depth_divisor\n",
        "    min_depth = global_params.min_depth\n",
        "    filters *= multiplier\n",
        "    min_depth = min_depth or divisor\n",
        "    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
        "    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n",
        "        new_filters += divisor\n",
        "    return int(new_filters)\n",
        "\n",
        "\n",
        "def round_repeats(repeats, global_params):\n",
        "    \"\"\" Round number of filters based on depth multiplier. \"\"\"\n",
        "    multiplier = global_params.depth_coefficient\n",
        "    if not multiplier:\n",
        "        return repeats\n",
        "    return int(math.ceil(multiplier * repeats))\n",
        "\n",
        "\n",
        "def drop_connect(inputs, p, training):\n",
        "    \"\"\" Drop connect. \"\"\"\n",
        "    if not training: return inputs\n",
        "    batch_size = inputs.shape[0]\n",
        "    keep_prob = 1 - p\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
        "    binary_tensor = torch.floor(random_tensor)\n",
        "    output = inputs / keep_prob * binary_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "def get_same_padding_conv2d(image_size=None):\n",
        "    \"\"\" Chooses static padding if you have specified an image size, and dynamic padding otherwise.\n",
        "        Static padding is necessary for ONNX exporting of models. \"\"\"\n",
        "    if image_size is None:\n",
        "        return Conv2dDynamicSamePadding\n",
        "    else:\n",
        "        return partial(Conv2dStaticSamePadding, image_size=image_size)\n",
        "\n",
        "\n",
        "class Conv2dDynamicSamePadding(nn.Conv2d):\n",
        "    \"\"\" 2D Convolutions like TensorFlow, for a dynamic image size \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
        "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        ih, iw = x.size()[-2:]\n",
        "        kh, kw = self.weight.size()[-2:]\n",
        "        sh, sw = self.stride\n",
        "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
        "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
        "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n",
        "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "class Conv2dStaticSamePadding(nn.Conv2d):\n",
        "    \"\"\" 2D Convolutions like TensorFlow, for a fixed image size\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, image_size=None, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, **kwargs)\n",
        "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
        "\n",
        "        # Calculate padding based on image size and save it\n",
        "        assert image_size is not None\n",
        "        ih, iw = image_size if type(image_size) == list else [image_size, image_size]\n",
        "        kh, kw = self.weight.size()[-2:]\n",
        "        sh, sw = self.stride\n",
        "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
        "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
        "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            self.static_padding = nn.ZeroPad2d((pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2))\n",
        "        else:\n",
        "            self.static_padding = Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.static_padding(x)\n",
        "        x = F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input\n",
        "\n",
        "\n",
        "########################################################################\n",
        "############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n",
        "########################################################################\n",
        "\n",
        "\n",
        "def efficientnet_params(model_name):\n",
        "    \"\"\" Map EfficientNet model name to parameter coefficients. \"\"\"\n",
        "    params_dict = {\n",
        "        # Coefficients:   width,depth,res,dropout\n",
        "        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
        "        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
        "        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
        "        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
        "        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
        "        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
        "        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
        "        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
        "    }\n",
        "    return params_dict[model_name]\n",
        "\n",
        "\n",
        "class BlockDecoder(object):\n",
        "    \"\"\" Block Decoder for readability, straight from the official TensorFlow repository \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _decode_block_string(block_string):\n",
        "        \"\"\" Gets a block through a string notation of arguments. \"\"\"\n",
        "        assert isinstance(block_string, str)\n",
        "\n",
        "        ops = block_string.split('_')\n",
        "        options = {}\n",
        "        for op in ops:\n",
        "            splits = re.split(r'(\\d.*)', op)\n",
        "            if len(splits) >= 2:\n",
        "                key, value = splits[:2]\n",
        "                options[key] = value\n",
        "\n",
        "        # Check stride\n",
        "        assert (('s' in options and len(options['s']) == 1) or\n",
        "                (len(options['s']) == 2 and options['s'][0] == options['s'][1]))\n",
        "\n",
        "        return BlockArgs(\n",
        "            kernel_size=int(options['k']),\n",
        "            num_repeat=int(options['r']),\n",
        "            input_filters=int(options['i']),\n",
        "            output_filters=int(options['o']),\n",
        "            expand_ratio=int(options['e']),\n",
        "            id_skip=('noskip' not in block_string),\n",
        "            se_ratio=float(options['se']) if 'se' in options else None,\n",
        "            stride=[int(options['s'][0])])\n",
        "\n",
        "    @staticmethod\n",
        "    def _encode_block_string(block):\n",
        "        \"\"\"Encodes a block to a string.\"\"\"\n",
        "        args = [\n",
        "            'r%d' % block.num_repeat,\n",
        "            'k%d' % block.kernel_size,\n",
        "            's%d%d' % (block.strides[0], block.strides[1]),\n",
        "            'e%s' % block.expand_ratio,\n",
        "            'i%d' % block.input_filters,\n",
        "            'o%d' % block.output_filters\n",
        "        ]\n",
        "        if 0 < block.se_ratio <= 1:\n",
        "            args.append('se%s' % block.se_ratio)\n",
        "        if block.id_skip is False:\n",
        "            args.append('noskip')\n",
        "        return '_'.join(args)\n",
        "\n",
        "    @staticmethod\n",
        "    def decode(string_list):\n",
        "        \"\"\"\n",
        "        Decodes a list of string notations to specify blocks inside the network.\n",
        "        :param string_list: a list of strings, each string is a notation of block\n",
        "        :return: a list of BlockArgs namedtuples of block args\n",
        "        \"\"\"\n",
        "        assert isinstance(string_list, list)\n",
        "        blocks_args = []\n",
        "        for block_string in string_list:\n",
        "            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n",
        "        return blocks_args\n",
        "\n",
        "    @staticmethod\n",
        "    def encode(blocks_args):\n",
        "        \"\"\"\n",
        "        Encodes a list of BlockArgs to a list of strings.\n",
        "        :param blocks_args: a list of BlockArgs namedtuples of block args\n",
        "        :return: a list of strings, each string is a notation of block\n",
        "        \"\"\"\n",
        "        block_strings = []\n",
        "        for block in blocks_args:\n",
        "            block_strings.append(BlockDecoder._encode_block_string(block))\n",
        "        return block_strings\n",
        "\n",
        "\n",
        "def efficientnet(width_coefficient=None, depth_coefficient=None, dropout_rate=0.2,\n",
        "                 drop_connect_rate=0.2, image_size=None, num_classes=1000):\n",
        "    \"\"\" Creates a efficientnet model. \"\"\"\n",
        "\n",
        "    blocks_args = [\n",
        "        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n",
        "        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n",
        "        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n",
        "        'r1_k3_s11_e6_i192_o320_se0.25',\n",
        "    ]\n",
        "    blocks_args = BlockDecoder.decode(blocks_args)\n",
        "\n",
        "    global_params = GlobalParams(\n",
        "        batch_norm_momentum=0.99,\n",
        "        batch_norm_epsilon=1e-3,\n",
        "        dropout_rate=dropout_rate,\n",
        "        drop_connect_rate=drop_connect_rate,\n",
        "        # data_format='channels_last',  # removed, this is always true in PyTorch\n",
        "        num_classes=num_classes,\n",
        "        width_coefficient=width_coefficient,\n",
        "        depth_coefficient=depth_coefficient,\n",
        "        depth_divisor=8,\n",
        "        min_depth=None,\n",
        "        image_size=image_size,\n",
        "    )\n",
        "\n",
        "    return blocks_args, global_params\n",
        "\n",
        "\n",
        "def get_model_params(model_name, override_params):\n",
        "    \"\"\" Get the block args and global params for a given model \"\"\"\n",
        "    if model_name.startswith('efficientnet'):\n",
        "        w, d, s, p = efficientnet_params(model_name)\n",
        "        # note: all models have drop connect rate = 0.2\n",
        "        blocks_args, global_params = efficientnet(\n",
        "            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s)\n",
        "    else:\n",
        "        raise NotImplementedError('model name is not pre-defined: %s' % model_name)\n",
        "    if override_params:\n",
        "        # ValueError will be raised here if override_params has fields not included in global_params.\n",
        "        global_params = global_params._replace(**override_params)\n",
        "    return blocks_args, global_params\n",
        "\n",
        "\n",
        "url_map = {\n",
        "    'efficientnet-b0': '/content/drive/My Drive/efficientnet-pytorch/efficientnet-b0-08094119.pth',\n",
        "    'efficientnet-b1': '/content/drive/My Drive/efficientnet-pytorch/efficientnet-b1-dbc7070a.pth',\n",
        "    'efficientnet-b2': '/content/drive/My Drive/efficientnet-pytorch/efficientnet-b2-27687264.pth',\n",
        "    'efficientnet-b3': '/content/drive/My Drive/efficientnet-pytorch/efficientnet-b3-c8376fa2.pth',\n",
        "    'efficientnet-b4': '/content/drive/My Drive/efficientnet-pytorch/efficientnet-b4-e116e8b3.pth',\n",
        "    'efficientnet-b5': '/content/drive/My Drive/efficientnet-pytorch/efficientnet-b5-586e6cc6.pth',\n",
        "    'efficientnet-b6': '/content/drive/My Drive/efficientnet-pytorch/efficientnet-b6-c76e70fd.pth',\n",
        "    'efficientnet-b7': '/content/drive/My Drive/efficientnet-pytorch/efficientnet-b7-dcc49843.pth',\n",
        "}\n",
        "\n",
        "## This below function is modified to use the pretrained weight for single channel . Its nothing but summing the weight across one axis .\n",
        "def load_pretrained_weights(model, model_name, load_fc=True,ch=1):\n",
        "    \"\"\" Loads pretrained weights, and downloads if loading for the first time. \"\"\"\n",
        "    state_dict = torch.load('/content/drive/My Drive/efficientnet-pytorch/efficientnet-b4-e116e8b3.pth')\n",
        "    if load_fc:\n",
        "        if ch == 1:\n",
        "            conv1_weight = state_dict['_conv_stem.weight']\n",
        "            state_dict['_conv_stem.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n",
        "        model.load_state_dict(state_dict)\n",
        "        \n",
        "    else:\n",
        "        state_dict.pop('_fc.weight')\n",
        "        state_dict.pop('_fc.bias')\n",
        "        if ch == 1:\n",
        "            conv1_weight = state_dict['_conv_stem.weight']\n",
        "            state_dict['_conv_stem.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n",
        "        res = model.load_state_dict(state_dict, strict=False)\n",
        "        print(res.missing_keys)\n",
        "        assert set(res.missing_keys) == set(['_fc.weight', '_fc.bias','fc1.weight', 'fc1.bias','fc2.weight', 'fc2.bias','fc3.weight', 'fc3.bias']), 'issue loading pretrained weights'\n",
        "    print('Loaded pretrained weights for {}'.format(model_name))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ChcrvRcO3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class MBConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Mobile Inverted Residual Bottleneck Block\n",
        "    Args:\n",
        "        block_args (namedtuple): BlockArgs, see above\n",
        "        global_params (namedtuple): GlobalParam, see above\n",
        "    Attributes:\n",
        "        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, block_args, global_params):\n",
        "        super().__init__()\n",
        "        self._block_args = block_args\n",
        "        self._bn_mom = 1 - global_params.batch_norm_momentum\n",
        "        self._bn_eps = global_params.batch_norm_epsilon\n",
        "        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n",
        "        self.id_skip = block_args.id_skip  # skip connection and drop connect\n",
        "\n",
        "        # Get static or dynamic convolution depending on image size\n",
        "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
        "\n",
        "        # Expansion phase\n",
        "        inp = self._block_args.input_filters  # number of input channels\n",
        "        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n",
        "        if self._block_args.expand_ratio != 1:\n",
        "            self._expand_conv = Conv2d(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
        "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
        "\n",
        "        # Depthwise convolution phase\n",
        "        k = self._block_args.kernel_size\n",
        "        s = self._block_args.stride\n",
        "        self._depthwise_conv = Conv2d(\n",
        "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n",
        "            kernel_size=k, stride=s, bias=False)\n",
        "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
        "\n",
        "        # Squeeze and Excitation layer, if desired\n",
        "        if self.has_se:\n",
        "            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n",
        "            self._se_reduce = Conv2d(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
        "            self._se_expand = Conv2d(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
        "\n",
        "        # Output phase\n",
        "        final_oup = self._block_args.output_filters\n",
        "        self._project_conv = Conv2d(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
        "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
        "        self._swish = MemoryEfficientSwish()\n",
        "\n",
        "    def forward(self, inputs, drop_connect_rate=None):\n",
        "        \"\"\"\n",
        "        :param inputs: input tensor\n",
        "        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n",
        "        :return: output of block\n",
        "        \"\"\"\n",
        "\n",
        "        # Expansion and Depthwise Convolution\n",
        "        x = inputs\n",
        "        if self._block_args.expand_ratio != 1:\n",
        "            x = self._swish(self._bn0(self._expand_conv(inputs)))\n",
        "        x = self._swish(self._bn1(self._depthwise_conv(x)))\n",
        "\n",
        "        # Squeeze and Excitation\n",
        "        if self.has_se:\n",
        "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
        "            x_squeezed = self._se_expand(self._swish(self._se_reduce(x_squeezed)))\n",
        "            x = torch.sigmoid(x_squeezed) * x\n",
        "\n",
        "        x = self._bn2(self._project_conv(x))\n",
        "\n",
        "        # Skip connection and drop connect\n",
        "        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n",
        "        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n",
        "            if drop_connect_rate:\n",
        "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
        "            x = x + inputs  # skip connection\n",
        "        return x\n",
        "\n",
        "    def set_swish(self, memory_efficient=True):\n",
        "        \"\"\"Sets swish function as memory efficient (for training) or standard (for export)\"\"\"\n",
        "        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n",
        "\n",
        "\n",
        "class EfficientNet(nn.Module):\n",
        "    \"\"\"\n",
        "    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n",
        "    Args:\n",
        "        blocks_args (list): A list of BlockArgs to construct blocks\n",
        "        global_params (namedtuple): A set of GlobalParams shared between blocks\n",
        "    Example:\n",
        "        model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, blocks_args=None, global_params=None):\n",
        "        super().__init__()\n",
        "        assert isinstance(blocks_args, list), 'blocks_args should be a list'\n",
        "        assert len(blocks_args) > 0, 'block args must be greater than 0'\n",
        "        self._global_params = global_params\n",
        "        self._blocks_args = blocks_args\n",
        "\n",
        "        # Get static or dynamic convolution depending on image size\n",
        "        Conv2d = get_same_padding_conv2d(image_size=global_params.image_size)\n",
        "\n",
        "        # Batch norm parameters\n",
        "        bn_mom = 1 - self._global_params.batch_norm_momentum\n",
        "        bn_eps = self._global_params.batch_norm_epsilon\n",
        "\n",
        "        # Stem\n",
        "        in_channels = 1  # rgb\n",
        "        out_channels = round_filters(32, self._global_params)  # number of output channels\n",
        "        self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
        "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
        "\n",
        "        # Build blocks\n",
        "        self._blocks = nn.ModuleList([])\n",
        "        for block_args in self._blocks_args:\n",
        "\n",
        "            # Update block input and output filters based on depth multiplier.\n",
        "            block_args = block_args._replace(\n",
        "                input_filters=round_filters(block_args.input_filters, self._global_params),\n",
        "                output_filters=round_filters(block_args.output_filters, self._global_params),\n",
        "                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n",
        "            )\n",
        "\n",
        "            # The first block needs to take care of stride and filter size increase.\n",
        "            self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
        "            if block_args.num_repeat > 1:\n",
        "                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n",
        "            for _ in range(block_args.num_repeat - 1):\n",
        "                self._blocks.append(MBConvBlock(block_args, self._global_params))\n",
        "\n",
        "        # Head\n",
        "        in_channels = block_args.output_filters  # output of final block\n",
        "        out_channels = round_filters(1280, self._global_params)\n",
        "        self._conv_head = Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
        "\n",
        "        # Final linear layer\n",
        "        self._avg_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "        self._dropout = nn.Dropout(self._global_params.dropout_rate)\n",
        "        self._fc = nn.Linear(out_channels, self._global_params.num_classes)\n",
        "        # vowel_diacritic\n",
        "        self.fc1 = nn.Linear(out_channels,11)\n",
        "        # grapheme_root\n",
        "        self.fc2 = nn.Linear(out_channels,168)\n",
        "        # consonant_diacritic\n",
        "        self.fc3 = nn.Linear(out_channels,7)\n",
        "        self._swish = MemoryEfficientSwish()\n",
        "\n",
        "    def set_swish(self, memory_efficient=True):\n",
        "        \"\"\"Sets swish function as memory efficient (for training) or standard (for export)\"\"\"\n",
        "        self._swish = MemoryEfficientSwish() if memory_efficient else Swish()\n",
        "        for block in self._blocks:\n",
        "            block.set_swish(memory_efficient)\n",
        "\n",
        "\n",
        "    def extract_features(self, inputs):\n",
        "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
        "\n",
        "        # Stem\n",
        "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
        "\n",
        "        # Blocks\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "\n",
        "        # Head\n",
        "        x = self._swish(self._bn1(self._conv_head(x)))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
        "        bs = inputs.size(0)\n",
        "        # Convolution layers\n",
        "        x = self.extract_features(inputs)\n",
        "\n",
        "        # Pooling and final linear layer\n",
        "        x = self._avg_pooling(x)\n",
        "        x = x.view(bs, -1)\n",
        "        x = self._dropout(x)\n",
        "       # x = self._fc(x)\n",
        "        x1 = self.fc1(x)\n",
        "        x2= self.fc2(x)\n",
        "        x3 = self.fc3(x)\n",
        "        return x1,x2,x3\n",
        "\n",
        "    @classmethod\n",
        "    def from_name(cls, model_name, override_params=None):\n",
        "        cls._check_model_name_is_valid(model_name)\n",
        "        blocks_args, global_params = get_model_params(model_name, override_params)\n",
        "        return cls(blocks_args, global_params)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_name, num_classes=1000, in_channels = 1):\n",
        "        model = cls.from_name(model_name, override_params={'num_classes': num_classes})\n",
        "        load_pretrained_weights(model, model_name, load_fc=False)\n",
        "        if in_channels != 3:\n",
        "            Conv2d = get_same_padding_conv2d(image_size = model._global_params.image_size)\n",
        "            out_channels = round_filters(32, model._global_params)\n",
        "            model._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
        "        return model\n",
        "    \n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_name, num_classes=1000):\n",
        "        model = cls.from_name(model_name, override_params={'num_classes': num_classes})\n",
        "        load_pretrained_weights(model, model_name, load_fc=False)\n",
        "\n",
        "        return model\n",
        "\n",
        "    @classmethod\n",
        "    def get_image_size(cls, model_name):\n",
        "        cls._check_model_name_is_valid(model_name)\n",
        "        _, _, res, _ = efficientnet_params(model_name)\n",
        "        return res\n",
        "\n",
        "    @classmethod\n",
        "    def _check_model_name_is_valid(cls, model_name, also_need_pretrained_weights=False):\n",
        "        \"\"\" Validates model name. None that pretrained weights are only available for\n",
        "        the first four models (efficientnet-b{i} for i in 0,1,2,3) at the moment. \"\"\"\n",
        "        num_models = 4 if also_need_pretrained_weights else 8\n",
        "        valid_models = ['efficientnet-b'+str(i) for i in range(num_models)]\n",
        "        if model_name not in valid_models:\n",
        "            raise ValueError('model_name should be one of: ' + ', '.join(valid_models))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJOYU7Jub0go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/nipuyo/train.csv')\n",
        "data0 = pd.read_feather('/content/drive/My Drive/ors_bengal/train_data_0_ors.feather')\n",
        "data1 = pd.read_feather('/content/drive/My Drive/ors_bengal/train_data_1_ors.feather')\n",
        "data2 = pd.read_feather('/content/drive/My Drive/ors_bengal/train_data_2_ors.feather')  \n",
        "data3 = pd.read_feather('/content/drive/My Drive/ors_bengal/train_data_3_ors.feather')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKQODstTbRlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_full = pd.concat([data0,data1,data2,data3],ignore_index=True)\n",
        "del data0,data1,data2,data3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPK0Avm4c7Ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GraphemeDataset(Dataset):\n",
        "    def __init__(self,df,label,_type='train',transform =False,aug = train_aug):\n",
        "        self.df = df\n",
        "        self.label = label\n",
        "        self.aug = aug\n",
        "        self.transform = transform\n",
        "        self.data = df.iloc[:, 1:].values\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self,idx):\n",
        "        label1 = self.label.vowel_diacritic.values[idx]\n",
        "        label2 = self.label.grapheme_root.values[idx]\n",
        "        label3 = self.label.consonant_diacritic.values[idx]\n",
        "        image = self.data[idx, :].reshape(my_size,my_size).astype(np.float)\n",
        "        if self.transform: \n",
        "            augment = self.aug(image = image)\n",
        "            image = augment['image']\n",
        "        return image,label1,label2,label3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f57kTxZVdFAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df , valid_df = train_test_split(train,test_size=0.20, random_state=42,shuffle=True) ## Split Labels\n",
        "data_train_df, data_valid_df = train_test_split(data_full,test_size=0.20, random_state=42,shuffle =True) ## split data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPOKdqzcdFL1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "63bf7c92-8ba9-4668-8e0c-bebd30007b69"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hV0GuCVdFPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "954cfdfa-b081-4f19-adde-9e7c3f6e609e"
      },
      "source": [
        "train_dataset = GraphemeDataset(data_train_df ,train_df,transform = False) \n",
        "valid_dataset = GraphemeDataset(data_valid_df ,valid_df,transform = False) \n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POy0B7pNdxWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "da77fb54-a658-4935-b0cf-957d96dac694"
      },
      "source": [
        "del train_df,valid_df,data_train_df,data_valid_df \n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOWhnIWldxZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrZYg1JHeJqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0a38771b-b014-440b-f20b-4ba121a1aa31"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = EfficientNet.from_pretrained('efficientnet-b4').to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/ors_bengal/four_only_cutmix_b4.pth'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "['_fc.weight', '_fc.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']\n",
            "Loaded pretrained weights for efficientnet-b4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwCgfGtZeJ6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=7.8125e-06)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                            mode=\"min\", \n",
        "                                                            patience=5, \n",
        "                                                            factor=0.5,verbose=True)\n",
        "batch_size=128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfJQNzxheJ9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sklearn.metrics\n",
        "import torch\n",
        "\n",
        "\n",
        "def macro_recall(pred_y, y, n_grapheme=168, n_vowel=11, n_consonant=7):\n",
        "    pred_y = torch.split(pred_y, [n_grapheme, n_vowel, n_consonant], dim=1)\n",
        "    pred_labels = [torch.argmax(py, dim=1).cpu().numpy() for py in pred_y]\n",
        "\n",
        "    y = y.cpu().numpy()\n",
        "    # pred_y = [p.cpu().numpy() for p in pred_y]\n",
        "\n",
        "    recall_grapheme = sklearn.metrics.recall_score(pred_labels[0], y[:, 0], average='macro')\n",
        "    recall_vowel = sklearn.metrics.recall_score(pred_labels[1], y[:, 1], average='macro')\n",
        "    recall_consonant = sklearn.metrics.recall_score(pred_labels[2], y[:, 2], average='macro')\n",
        "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
        "    final_score = np.average(scores, weights=[2, 1, 1])\n",
        "    # print(f'recall: grapheme {recall_grapheme}, vowel {recall_vowel}, consonant {recall_consonant}, '\n",
        "    #       f'total {final_score}, y {y.shape}')\n",
        "    return final_score\n",
        "\n",
        "def macro_recall_multi(pred_graphemes, true_graphemes,pred_vowels,true_vowels,pred_consonants,true_consonants, n_grapheme=168, n_vowel=11, n_consonant=7):\n",
        "    #pred_y = torch.split(pred_y, [n_grapheme], dim=1)\n",
        "    pred_label_graphemes = torch.argmax(pred_graphemes, dim=1).cpu().numpy()\n",
        "\n",
        "    true_label_graphemes = true_graphemes.cpu().numpy()\n",
        "    \n",
        "    pred_label_vowels = torch.argmax(pred_vowels, dim=1).cpu().numpy()\n",
        "\n",
        "    true_label_vowels = true_vowels.cpu().numpy()\n",
        "    \n",
        "    pred_label_consonants = torch.argmax(pred_consonants, dim=1).cpu().numpy()\n",
        "\n",
        "    true_label_consonants = true_consonants.cpu().numpy()    \n",
        "    # pred_y = [p.cpu().numpy() for p in pred_y]\n",
        "\n",
        "    recall_grapheme = sklearn.metrics.recall_score(pred_label_graphemes, true_label_graphemes, average='macro')\n",
        "    recall_vowel = sklearn.metrics.recall_score(pred_label_vowels, true_label_vowels, average='macro')\n",
        "    recall_consonant = sklearn.metrics.recall_score(pred_label_consonants, true_label_consonants, average='macro')\n",
        "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
        "    final_score = np.average(scores, weights=[2, 1, 1])\n",
        "    #print(f'recall: grapheme {recall_grapheme}, vowel {recall_vowel}, consonant {recall_consonant}, '\n",
        "    #       f'total {final_score}')\n",
        "    return final_score\n",
        "\n",
        "\n",
        "def calc_macro_recall(solution, submission):\n",
        "    # solution df, submission df\n",
        "    scores = []\n",
        "    for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n",
        "        y_true_subset = solution[solution[component] == component]['target'].values\n",
        "        y_pred_subset = submission[submission[component] == component]['target'].values\n",
        "        scores.append(sklearn.metrics.recall_score(\n",
        "            y_true_subset, y_pred_subset, average='macro'))\n",
        "    final_score = np.average(scores, weights=[2, 1, 1])\n",
        "    return final_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYRn-lx3M3U6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ohem_loss( rate, cls_pred, cls_target ):\n",
        "\n",
        "    batch_size = cls_pred.size(0) \n",
        "    ohem_cls_loss = F.cross_entropy(cls_pred, cls_target, reduction='none', ignore_index=-1)\n",
        "\n",
        "    sorted_ohem_loss, idx = torch.sort(ohem_cls_loss, descending=True)\n",
        "    keep_num = min(sorted_ohem_loss.size()[0], int(batch_size*rate) )\n",
        "    if keep_num < sorted_ohem_loss.size()[0]:\n",
        "        keep_idx_cuda = idx[:keep_num]\n",
        "        ohem_cls_loss = ohem_cls_loss[keep_idx_cuda]\n",
        "    cls_loss = ohem_cls_loss.sum() / keep_num\n",
        "    return cls_loss\n",
        "\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "def cutmix(data, targets1, targets2, targets3, alpha):\n",
        "    indices = torch.randperm(data.size(0))\n",
        "    shuffled_data = data[indices]\n",
        "    shuffled_targets1 = targets1[indices]\n",
        "    shuffled_targets2 = targets2[indices]\n",
        "    shuffled_targets3 = targets3[indices]\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
        "    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n",
        "    # adjust lambda to exactly match pixel ratio\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
        "\n",
        "    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3, shuffled_targets3, lam]\n",
        "    return data, targets\n",
        "# loss \n",
        "def cutmix_criterion(preds1,preds2,preds3, targets, rate_ohem):\n",
        "    #print('change:',rate_ohem)\n",
        "    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n",
        "    #criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "    criterion = ohem_loss\n",
        "    lo1 = lam * criterion(rate_ohem,preds1, targets1) + (1 - lam) * criterion(rate_ohem,preds1, targets2)\n",
        "    lo2 = lam * criterion(rate_ohem,preds2, targets3) + (1 - lam) * criterion(rate_ohem,preds2, targets4)\n",
        "    lo3 = lam * criterion(rate_ohem,preds3, targets5) + (1 - lam) * criterion(rate_ohem,preds3, targets6)\n",
        "    #lo1 = lam * criterion(preds1, targets1) + (1 - lam) * criterion(preds1, targets2)\n",
        "    #lo2 = lam * criterion(preds2, targets3) + (1 - lam) * criterion(preds2, targets4)\n",
        "    #lo3 = lam * criterion(preds3, targets5) + (1 - lam) * criterion(preds3, targets6)\n",
        "    return lo1,lo2,lo3\n",
        "\n",
        "\n",
        "def mixup(data, targets1, targets2, targets3, alpha):\n",
        "    indices = torch.randperm(data.size(0))\n",
        "    shuffled_data = data[indices]\n",
        "    shuffled_targets1 = targets1[indices]\n",
        "    shuffled_targets2 = targets2[indices]\n",
        "    shuffled_targets3 = targets3[indices]\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    data = data * lam + shuffled_data * (1 - lam)\n",
        "    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3, shuffled_targets3, lam]\n",
        "\n",
        "    return data, targets\n",
        "\n",
        "\n",
        "def mixup_criterion(preds1,preds2,preds3, targets, rate=0.7):\n",
        "    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n",
        "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "    #criterion = ohem_loss\n",
        "    lo1 = lam * criterion(preds1, targets1) + (1 - lam) * criterion(preds1, targets2)\n",
        "    lo2 = lam * criterion(preds2, targets3) + (1 - lam) * criterion(preds2, targets4)\n",
        "    lo3 = lam * criterion(preds3, targets5) + (1 - lam) * criterion(preds3, targets6)\n",
        "    return  lo1,lo2,lo3 \n",
        "    #return [ lam * criterion(rate, preds1, targets1) + (1 - lam) * criterion(rate, preds1, targets2), lam * criterion(rate, preds2, targets3) + (1 - lam) * criterion(rate, preds2, targets4), lam * criterion(rate, preds3, targets5) + (1 - lam) * criterion(rate, preds3, targets6) ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEjTIuYBGCVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criter = ohem_loss\n",
        "#criter = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JioYBPLMdDF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "def train(epoch,rate_ohem):\n",
        "  model.train()\n",
        "  losses = []\n",
        "  accs = []\n",
        "  acc= 0.0\n",
        "  total = 0.0\n",
        "  running_loss = 0.0\n",
        "  running_acc = 0.0\n",
        "  valid_recall = 0.0\n",
        "  running_recall = 0.0\n",
        "  rate_ohem = rate_ohem\n",
        "  for idx, (inputs,labels1,labels2,labels3) in enumerate(train_loader):\n",
        "      inputs = inputs.unsqueeze(1).float()\n",
        "      inputs = inputs.to(device)\n",
        "      labels1 = labels1.to(device)\n",
        "      labels2 = labels2.to(device)\n",
        "      labels3 = labels3.to(device)\n",
        "      total += len(inputs)\n",
        "      optimizer.zero_grad()\n",
        "      inputs, targets = cutmix(inputs, labels1, labels2, labels3, 0.6)\n",
        "      outputs1,outputs2,outputs3 = model(inputs)\n",
        "      loss1,loss2,loss3 = cutmix_criterion(outputs1,outputs2,outputs3, targets,rate_ohem)  \n",
        "      running_recall+= macro_recall_multi(outputs2,labels2,outputs1,labels1,outputs3,labels3)\n",
        "      running_loss += loss1.item()+loss2.item()+loss3.item()\n",
        "      running_acc += (outputs1.argmax(1)==labels1).float().mean()\n",
        "      running_acc += (outputs2.argmax(1)==labels2).float().mean()\n",
        "      running_acc += (outputs3.argmax(1)==labels3).float().mean()\n",
        "      (loss1+loss2+loss3).backward()\n",
        "      optimizer.step()\n",
        "      acc = running_acc/total\n",
        "  \n",
        "  losses.append(running_loss/len(train_loader))\n",
        "  accs.append(running_acc/(len(train_loader)*3))\n",
        "  print(' train epoch : {}'.format(epoch))\n",
        "  print('loss : {:.4f}'.format(running_loss/len(train_loader)))\n",
        "  print('recall: {:.4f}'.format(running_recall/len(train_loader)))\n",
        "  total_train_recall = running_recall/len(train_loader)\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return total_train_recall\n",
        "    \n",
        "def evaluate(epoch,rate_ohem):\n",
        "   model.eval()\n",
        "   losses = []\n",
        "   accs = []\n",
        "   recalls = []\n",
        "   acc= 0.0\n",
        "   total = 0.0\n",
        "   #print('epochs {}/{} '.format(epoch+1,epochs))\n",
        "   running_loss = 0.0\n",
        "   running_acc = 0.0\n",
        "   running_recall = 0.0\n",
        "   rate_ohem = rate_ohem\n",
        "   with torch.no_grad():\n",
        "     for idx, (inputs,labels1,labels2,labels3) in enumerate(valid_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels1 = labels1.to(device)\n",
        "        labels2 = labels2.to(device)\n",
        "        labels3 = labels3.to(device)\n",
        "        total += len(inputs)\n",
        "        outputs1,outputs2,outputs3 = model(inputs.unsqueeze(1).float())\n",
        "        loss1 = criter(rate_ohem,outputs1,labels1)\n",
        "        loss2 = 2*criter(rate_ohem,outputs2,labels2)\n",
        "        loss3 = criter(rate_ohem,outputs3,labels3)\n",
        "        #loss1 = criter(outputs1,labels1)\n",
        "        #loss2 = 2*criter(outputs2,labels2)\n",
        "        #loss3 = criter(outputs3,labels3)\n",
        "        running_loss += loss1.item()+loss2.item()+loss3.item()\n",
        "        running_recall+= macro_recall_multi(outputs2,labels2,outputs1,labels1,outputs3,labels3)\n",
        "        running_acc += (outputs1.argmax(1)==labels1).float().mean()\n",
        "        running_acc += (outputs2.argmax(1)==labels2).float().mean()\n",
        "        running_acc += (outputs3.argmax(1)==labels3).float().mean()\n",
        "        acc = running_acc/total\n",
        "   shed_rec = running_loss/len(valid_loader)\n",
        "   scheduler.step(shed_rec)\n",
        "   losses.append(running_loss/len(valid_loader))\n",
        "   accs.append(running_acc/(len(valid_loader)*3))\n",
        "   recalls.append(running_recall/len(valid_loader))\n",
        "   total_recall = running_recall/len(valid_loader)\n",
        "   loss = running_loss/len(valid_loader)\n",
        "   print('val epoch: {}'.format(epoch))\n",
        "   print('loss : {:.4f}'.format(running_loss/len(valid_loader)))\n",
        "   print('recall: {:.4f}'.format(running_recall/len(valid_loader)))\n",
        "   return  total_recall,loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dYL8D1KdITu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "91774417-fd62-45c2-8712-2e39951c23cd"
      },
      "source": [
        "#started new :)\n",
        "#16:30 started training \n",
        "n_epochs = 40\n",
        "valid_recall = 0.0 \n",
        "best_valid_recall = 0.9889\n",
        "best_valid_loss = 0.1467\n",
        "for epoch in range(n_epochs): \n",
        "  if epoch < 5:\n",
        "    rate_ohem = 1\n",
        "  elif epoch < 10:\n",
        "    rate_ohem = 0.5\n",
        "  elif epoch < 15:\n",
        "    rate_ohem = 1/3\n",
        "  elif epoch < 20:\n",
        "    rate_ohem = 1/4\n",
        "  elif epoch < 25:\n",
        "    rate_ohem = 1/5\n",
        "  elif epoch < 30:\n",
        "    rate_ohem = 1/6\n",
        "  elif epoch < 35:\n",
        "    rate_ohem = 1/7\n",
        "  else:\n",
        "    rate_ohem = 1/8\n",
        "  print('--'*50)  \n",
        "  print(f\"current ohem rate is :{rate_ohem}\")\n",
        "  torch.cuda.empty_cache() \n",
        "  gc.collect()\n",
        "  train_recall = train(epoch,rate_ohem)\n",
        "  valid_recall,valid_loss = evaluate(epoch,rate_ohem)\n",
        "  if valid_recall > best_valid_recall: \n",
        "        print(f'Validation recall has increased from:  {best_valid_recall:.4f} to: {valid_recall:.4f}. Saving checkpoint')\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/ors_bengal/five_only_cutmix_b4.pth') \n",
        "        best_valid_recall = valid_recall\n",
        "  elif valid_loss < best_valid_loss: \n",
        "        print(f'Validation loss has decreased from:  {best_valid_loss:.4f} to: {valid_loss:.4f}. Saving checkpoint')\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/ors_bengal/loss_model_b4.pth') \n",
        "        best_valid_loss = valid_loss\n",
        "  elif valid_recall > 0.9885:\n",
        "        print('Saving model which is greater than 0.9885')\n",
        "        torch.save(model.state_dict(), f'/content/drive/My Drive/all_models/b4_{valid_recall}.pth')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "current ohem rate is :1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: DeprecationWarning: 'saved_variables' is deprecated; use 'saved_tensors'\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " train epoch : 0\n",
            "loss : 2.3568\n",
            "recall: 0.6876\n",
            "val epoch: 0\n",
            "loss : 0.1457\n",
            "recall: 0.9884\n",
            "Validation loss has decreased from:  0.1467 to: 0.1457. Saving checkpoint\n",
            "----------------------------------------------------------------------------------------------------\n",
            "current ohem rate is :1\n",
            " train epoch : 1\n",
            "loss : 2.3162\n",
            "recall: 0.7062\n",
            "val epoch: 1\n",
            "loss : 0.1458\n",
            "recall: 0.9882\n",
            "----------------------------------------------------------------------------------------------------\n",
            "current ohem rate is :1\n",
            " train epoch : 2\n",
            "loss : 2.3216\n",
            "recall: 0.7189\n",
            "val epoch: 2\n",
            "loss : 0.1457\n",
            "recall: 0.9886\n",
            "Validation loss has decreased from:  0.1457 to: 0.1457. Saving checkpoint\n",
            "----------------------------------------------------------------------------------------------------\n",
            "current ohem rate is :1\n",
            " train epoch : 3\n",
            "loss : 2.3462\n",
            "recall: 0.7103\n",
            "val epoch: 3\n",
            "loss : 0.1458\n",
            "recall: 0.9886\n",
            "Saving model which is greater than 0.9885\n",
            "----------------------------------------------------------------------------------------------------\n",
            "current ohem rate is :1\n",
            " train epoch : 4\n",
            "loss : 2.3414\n",
            "recall: 0.7005\n",
            "val epoch: 4\n",
            "loss : 0.1470\n",
            "recall: 0.9875\n",
            "----------------------------------------------------------------------------------------------------\n",
            "current ohem rate is :0.5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}