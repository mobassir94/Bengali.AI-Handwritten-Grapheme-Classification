{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wideresnet50.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3da49d2901c74f5bb7d9f6a02e51e005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b4f673081a29415e8a352fe67abfee67",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f042dcd9d2a44793807c6e5c50ea20a3",
              "IPY_MODEL_73316d3d2cd247d6879e43b7daca77ee"
            ]
          }
        },
        "b4f673081a29415e8a352fe67abfee67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f042dcd9d2a44793807c6e5c50ea20a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_232ac77d096d4a5e9af6ccb415c89d96",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 202,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 202,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ad4c050b4fcc46ed87749493fd8e2ff1"
          }
        },
        "73316d3d2cd247d6879e43b7daca77ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8e0e3b5f8a904c28a4fa1f6df520e1be",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 202/202 [00:29&lt;00:00,  6.75it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a6f811aa8c540aeb24b1fda60393191"
          }
        },
        "232ac77d096d4a5e9af6ccb415c89d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ad4c050b4fcc46ed87749493fd8e2ff1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e0e3b5f8a904c28a4fa1f6df520e1be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a6f811aa8c540aeb24b1fda60393191": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2930fcf0d1e74c488cfe2c17cdd91768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e51956b0c11845cbb0a5273403271284",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_88e68bb064e04b84a471e811545c6403",
              "IPY_MODEL_969d27fd32fb452c83b864fcb7d00429"
            ]
          }
        },
        "e51956b0c11845cbb0a5273403271284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88e68bb064e04b84a471e811545c6403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_643fa59fb2994756adee4de0e13304a3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 202,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 153,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d777e888339949b086c43ac254be0869"
          }
        },
        "969d27fd32fb452c83b864fcb7d00429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_15423d88752d4c14901f8750957d0112",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 76% 153/202 [00:22&lt;00:07,  6.27it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a16361ea565f4ca19a93861ce020eba0"
          }
        },
        "643fa59fb2994756adee4de0e13304a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d777e888339949b086c43ac254be0869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "15423d88752d4c14901f8750957d0112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a16361ea565f4ca19a93861ce020eba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhIPwSQxVZGp",
        "colab_type": "code",
        "outputId": "ed07e3fd-136f-40a0-c802-df603ca9687c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JWDi7wM7bjX",
        "colab_type": "code",
        "outputId": "819dc7ab-5032-4829-f262-dbb934e12afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        }
      },
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/albu/albumentations\n",
            "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-jtavjjwg\n",
            "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-jtavjjwg\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.4) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.4) (1.4.1)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.4) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.4) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (2.4)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (3.1.3)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (6.2.2)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (2.4.6)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.4) (45.1.0)\n",
            "Building wheels for collected packages: albumentations, imgaug\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.4-cp36-none-any.whl size=64137 sha256=3d33fbf4c53448d262dcc62467ba48740cf22cdbe50e3fd2ce84b92c4f3022ed\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y2kf4bwq/wheels/45/8b/e4/2837bbcf517d00732b8e394f8646f22b8723ac00993230188b\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654020 sha256=7b6a6738835bfe210c6d556bcf6a58552596168980ad391d0a848af001b5cb98\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built albumentations imgaug\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.4 imgaug-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0MaY8R88FBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division, absolute_import\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchvision import transforms,models\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import albumentations\n",
        "from PIL import Image, ImageOps, ImageEnhance\n",
        "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
        "from albumentations.augmentations import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "\n",
        "from albumentations import (\n",
        "    PadIfNeeded,\n",
        "    HorizontalFlip,\n",
        "    VerticalFlip,    \n",
        "    CenterCrop,    \n",
        "    Crop,\n",
        "    Compose,\n",
        "    Transpose,\n",
        "    RandomRotate90,\n",
        "    ElasticTransform,\n",
        "    GridDistortion, \n",
        "    OpticalDistortion,\n",
        "    RandomSizedCrop,\n",
        "    OneOf,\n",
        "    CLAHE,\n",
        "    RandomBrightnessContrast,    \n",
        "    RandomGamma,\n",
        "    ShiftScaleRotate ,\n",
        "    GaussNoise,\n",
        "    Blur,\n",
        "    MotionBlur\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXv3_H3H8FPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Cutout:\n",
        "    def __init__(self, mask_size, p, cutout_inside, mask_color=1):\n",
        "        self.p = p\n",
        "        self.mask_size = mask_size\n",
        "        self.cutout_inside = cutout_inside\n",
        "        self.mask_color = mask_color\n",
        "\n",
        "        self.mask_size_half = mask_size // 2\n",
        "        self.offset = 1 if mask_size % 2 == 0 else 0\n",
        "\n",
        "    def __call__(self, image):\n",
        "        image = np.asarray(image).copy()\n",
        "\n",
        "        if np.random.random() > self.p:\n",
        "            return image\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "\n",
        "        if self.cutout_inside:\n",
        "            cxmin, cxmax = self.mask_size_half, w + self.offset - self.mask_size_half\n",
        "            cymin, cymax = self.mask_size_half, h + self.offset - self.mask_size_half\n",
        "        else:\n",
        "            cxmin, cxmax = 0, w + self.offset\n",
        "            cymin, cymax = 0, h + self.offset\n",
        "\n",
        "        cx = np.random.randint(cxmin, cxmax)\n",
        "        cy = np.random.randint(cymin, cymax)\n",
        "        xmin = cx - self.mask_size_half\n",
        "        ymin = cy - self.mask_size_half\n",
        "        xmax = xmin + self.mask_size\n",
        "        ymax = ymin + self.mask_size\n",
        "        xmin = max(0, xmin)\n",
        "        ymin = max(0, ymin)\n",
        "        xmax = min(w, xmax)\n",
        "        ymax = min(h, ymax)\n",
        "        image[ymin:ymax, xmin:xmax] = self.mask_color\n",
        "        return image\n",
        "\n",
        "\n",
        "class DualCutout:\n",
        "    def __init__(self, mask_size, p, cutout_inside, mask_color=1):\n",
        "        self.cutout = Cutout(mask_size, p, cutout_inside, mask_color)\n",
        "\n",
        "    def __call__(self, image):\n",
        "        return np.hstack([self.cutout(image), self.cutout(image)])\n",
        "\n",
        "\n",
        "class DualCutoutCriterion:\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha = alpha\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "\n",
        "    def __call__(self, preds, targets):\n",
        "        preds1, preds2 = preds\n",
        "        return (self.criterion(preds1, targets) + self.criterion(\n",
        "            preds2, targets)) * 0.5 + self.alpha * F.mse_loss(preds1, preds2)\n",
        "\n",
        "\n",
        "def mixup(data, targets, alpha, n_classes):\n",
        "    indices = torch.randperm(data.size(0))\n",
        "    shuffled_data = data[indices]\n",
        "    shuffled_targets = targets[indices]\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    data = data * lam + shuffled_data * (1 - lam)\n",
        "    targets = (targets, shuffled_targets, lam)\n",
        "\n",
        "    return data, targets\n",
        "\n",
        "\n",
        "def mixup_criterion(preds, targets):\n",
        "    targets1, targets2, lam = targets\n",
        "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "    return lam * criterion(preds, targets1) + (1 - lam) * criterion(\n",
        "        preds, targets2)\n",
        "    \n",
        "\n",
        "\n",
        "class RandomErasing:\n",
        "    def __init__(self, p, area_ratio_range, min_aspect_ratio, max_attempt):\n",
        "        self.p = p\n",
        "        self.max_attempt = max_attempt\n",
        "        self.sl, self.sh = area_ratio_range\n",
        "        self.rl, self.rh = min_aspect_ratio, 1. / min_aspect_ratio\n",
        "\n",
        "    def __call__(self, image):\n",
        "        image = np.asarray(image).copy()\n",
        "\n",
        "        if np.random.random() > self.p:\n",
        "            return image\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        image_area = h * w\n",
        "\n",
        "        for _ in range(self.max_attempt):\n",
        "            mask_area = np.random.uniform(self.sl, self.sh) * image_area\n",
        "            aspect_ratio = np.random.uniform(self.rl, self.rh)\n",
        "            mask_h = int(np.sqrt(mask_area * aspect_ratio))\n",
        "            mask_w = int(np.sqrt(mask_area / aspect_ratio))\n",
        "\n",
        "            if mask_w < w and mask_h < h:\n",
        "                x0 = np.random.randint(0, w - mask_w)\n",
        "                y0 = np.random.randint(0, h - mask_h)\n",
        "                x1 = x0 + mask_w\n",
        "                y1 = y0 + mask_h\n",
        "                image[y0:y1, x0:x1] = np.random.uniform(0, 1)\n",
        "                break\n",
        "\n",
        "        return image  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deiOBp2grfqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def int_parameter(level, maxval):\n",
        "    \"\"\"Helper function to scale `val` between 0 and maxval .\n",
        "    Args:\n",
        "    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
        "    maxval: Maximum value that the operation can have. This will be scaled to\n",
        "      level/PARAMETER_MAX.\n",
        "    Returns:\n",
        "    An int that results from scaling `maxval` according to `level`.\n",
        "    \"\"\"\n",
        "    return int(level * maxval / 10)\n",
        "\n",
        "\n",
        "def float_parameter(level, maxval):\n",
        "    \"\"\"Helper function to scale `val` between 0 and maxval.\n",
        "    Args:\n",
        "    level: Level of the operation that will be between [0, `PARAMETER_MAX`].\n",
        "    maxval: Maximum value that the operation can have. This will be scaled to\n",
        "      level/PARAMETER_MAX.\n",
        "    Returns:\n",
        "    A float that results from scaling `maxval` according to `level`.\n",
        "    \"\"\"\n",
        "    return float(level) * maxval / 10.\n",
        "\n",
        "\n",
        "def sample_level(n):\n",
        "    return np.random.uniform(low=0.1, high=n)\n",
        "\n",
        "\n",
        "def autocontrast(pil_img, _):\n",
        "    return ImageOps.autocontrast(pil_img)\n",
        "\n",
        "\n",
        "def equalize(pil_img, _):\n",
        "    return ImageOps.equalize(pil_img)\n",
        "\n",
        "\n",
        "def posterize(pil_img, level):\n",
        "    level = int_parameter(sample_level(level), 4)\n",
        "    return ImageOps.posterize(pil_img, 4 - level)\n",
        "\n",
        "\n",
        "def rotate(pil_img, level):\n",
        "    degrees = int_parameter(sample_level(level), 30)\n",
        "    if np.random.uniform() > 0.5:\n",
        "        degrees = -degrees\n",
        "    return pil_img.rotate(degrees, resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "def solarize(pil_img, level):\n",
        "    level = int_parameter(sample_level(level), 256)\n",
        "    return ImageOps.solarize(pil_img, 256 - level)\n",
        "\n",
        "\n",
        "def shear_x(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 0.3)\n",
        "    if np.random.uniform() > 0.5:\n",
        "        level = -level\n",
        "    return pil_img.transform(pil_img.size,\n",
        "                           Image.AFFINE, (1, level, 0, 0, 1, 0),\n",
        "                           resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "def shear_y(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 0.3)\n",
        "    if np.random.uniform() > 0.5:\n",
        "        level = -level\n",
        "    return pil_img.transform(pil_img.size,\n",
        "                           Image.AFFINE, (1, 0, 0, level, 1, 0),\n",
        "                           resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "def translate_x(pil_img, level):\n",
        "    level = int_parameter(sample_level(level), pil_img.size[0] / 3)\n",
        "    if np.random.random() > 0.5:\n",
        "        level = -level\n",
        "    return pil_img.transform(pil_img.size,\n",
        "                           Image.AFFINE, (1, 0, level, 0, 1, 0),\n",
        "                           resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "def translate_y(pil_img, level):\n",
        "    level = int_parameter(sample_level(level), pil_img.size[0] / 3)\n",
        "    if np.random.random() > 0.5:\n",
        "        level = -level\n",
        "    return pil_img.transform(pil_img.size,\n",
        "                           Image.AFFINE, (1, 0, 0, 0, 1, level),\n",
        "                           resample=Image.BILINEAR)\n",
        "\n",
        "\n",
        "# operation that overlaps with ImageNet-C's test set\n",
        "def color(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
        "    return ImageEnhance.Color(pil_img).enhance(level)\n",
        "\n",
        "\n",
        "# operation that overlaps with ImageNet-C's test set\n",
        "def contrast(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
        "    return ImageEnhance.Contrast(pil_img).enhance(level)\n",
        "\n",
        "\n",
        "# operation that overlaps with ImageNet-C's test set\n",
        "def brightness(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
        "    return ImageEnhance.Brightness(pil_img).enhance(level)\n",
        "\n",
        "\n",
        "# operation that overlaps with ImageNet-C's test set\n",
        "def sharpness(pil_img, level):\n",
        "    level = float_parameter(sample_level(level), 1.8) + 0.1\n",
        "    return ImageEnhance.Sharpness(pil_img).enhance(level)\n",
        "\n",
        "\n",
        "augmentations = [\n",
        "    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n",
        "    translate_x, translate_y\n",
        "]\n",
        "\n",
        "augmentations_all = [\n",
        "    autocontrast, equalize, posterize, rotate, solarize, shear_x, shear_y,\n",
        "    translate_x, translate_y, color, contrast, brightness, sharpness\n",
        "]\n",
        "\n",
        "def normalize(image):\n",
        "    \"\"\"Normalize input image channel-wise to zero mean and unit variance.\"\"\"\n",
        "    return image - 127\n",
        "\n",
        "def apply_op(image, op, severity):\n",
        "    #   image = np.clip(image, 0, 255)\n",
        "    pil_img = Image.fromarray(image)  # Convert to PIL.Image\n",
        "    pil_img = op(pil_img, severity)\n",
        "    return np.asarray(pil_img)\n",
        "\n",
        "def augment_and_mix(image, severity=3, width=3, depth=-1, alpha=1.):\n",
        "    \"\"\"Perform AugMix augmentations and compute mixture.\n",
        "    Args:\n",
        "    image: Raw input image as float32 np.ndarray of shape (h, w, c)\n",
        "    severity: Severity of underlying augmentation operators (between 1 to 10).\n",
        "    width: Width of augmentation chain\n",
        "    depth: Depth of augmentation chain. -1 enables stochastic depth uniformly\n",
        "      from [1, 3]\n",
        "    alpha: Probability coefficient for Beta and Dirichlet distributions.\n",
        "    Returns:\n",
        "    mixed: Augmented and mixed image.\n",
        "    \"\"\"\n",
        "    ws = np.float32(\n",
        "      np.random.dirichlet([alpha] * width))\n",
        "    m = np.float32(np.random.beta(alpha, alpha))\n",
        "\n",
        "    mix = np.zeros_like(image).astype(np.float32)\n",
        "    for i in range(width):\n",
        "        image_aug = image.copy()\n",
        "        depth = depth if depth > 0 else np.random.randint(1, 4)\n",
        "        for _ in range(depth):\n",
        "            op = np.random.choice(augmentations)\n",
        "            image_aug = apply_op(image_aug, op, severity)\n",
        "        # Preprocessing commutes since all coefficients are convex\n",
        "        mix += ws[i] * image_aug\n",
        "#         mix += ws[i] * normalize(image_aug)\n",
        "\n",
        "    mixed = (1 - m) * image + m * mix\n",
        "#     mixed = (1 - m) * normalize(image) + m * mix\n",
        "    return mixed\n",
        "\n",
        "\n",
        "class RandomAugMix(ImageOnlyTransform):\n",
        "\n",
        "    def __init__(self, severity=3, width=3, depth=-1, alpha=1., always_apply=False, p=0.5):\n",
        "        super().__init__(always_apply, p)\n",
        "        self.severity = severity\n",
        "        self.width = width\n",
        "        self.depth = depth\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def apply(self, image, **params):\n",
        "        image = augment_and_mix(\n",
        "            image,\n",
        "            self.severity,\n",
        "            self.width,\n",
        "            self.depth,\n",
        "            self.alpha\n",
        "        )\n",
        "        return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPqyITBq8FSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/bengaliaicv19feather/train.csv')\n",
        "data0 = pd.read_feather('/content/drive/My Drive/bengaliaicv19feather/train_data_0.feather')\n",
        "data1 = pd.read_feather('/content/drive/My Drive/bengaliaicv19feather/train_data_1.feather')\n",
        "data2 = pd.read_feather('/content/drive/My Drive/bengaliaicv19feather/train_data_2.feather')\n",
        "data3 = pd.read_feather('/content/drive/My Drive/bengaliaicv19feather/train_data_3.feather')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4N9few18FVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_full = pd.concat([data0,data1,data2,data3],ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rODJusSh8FXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "train_aug = Compose([ \n",
        "    ShiftScaleRotate(p=1,border_mode=cv2.BORDER_CONSTANT,value =1),\n",
        "    OneOf([\n",
        "        ElasticTransform(p=0.1, alpha=1, sigma=50, alpha_affine=50,border_mode=cv2.BORDER_CONSTANT,value =1),\n",
        "        GridDistortion(distort_limit =0.05 ,border_mode=cv2.BORDER_CONSTANT,value =1, p=0.1),\n",
        "        OpticalDistortion(p=0.1, distort_limit= 0.05, shift_limit=0.2,border_mode=cv2.BORDER_CONSTANT,value =1)                  \n",
        "        ], p=0.3),\n",
        "    OneOf([\n",
        "        GaussNoise(var_limit=1.0),\n",
        "        Blur(),\n",
        "        #GaussianBlur(blur_limit=3)\n",
        "        ], p=0.4),    \n",
        "    RandomGamma(p=0.8)\n",
        "  \n",
        "    #RandomAugMix(severity=3, width=3, alpha=1., p=1.),\n",
        "\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJUVeI668Fa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ToTensor:\n",
        "    def __call__(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            return tuple([self._to_tensor(image) for image in data])\n",
        "        else:\n",
        "            return self._to_tensor(data)\n",
        "\n",
        "    def _to_tensor(self, data):\n",
        "        if len(data.shape) == 3:\n",
        "            return torch.from_numpy(data.transpose(2, 0, 1).astype(np.float32))\n",
        "        else:\n",
        "            return torch.from_numpy(data[None, :, :].astype(np.float32))\n",
        "\n",
        "\n",
        "class Normalize:\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = np.array(mean)\n",
        "        self.std = np.array(std)\n",
        "\n",
        "    def __call__(self, image):\n",
        "        image = np.asarray(image).astype(np.float32) / 255.\n",
        "        image = (image - self.mean) / self.std\n",
        "        return image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRkB7VE8B9k3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GraphemeDataset(Dataset):\n",
        "    def __init__(self,df,label,_type='train',transform =True,aug=train_aug):\n",
        "        self.df = df\n",
        "        self.label = label\n",
        "        self.aug = aug\n",
        "        self.transform = transform\n",
        "        self.data = df.iloc[:, 1:].values\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self,idx):\n",
        "        label1 = self.label.vowel_diacritic.values[idx]\n",
        "        label2 = self.label.grapheme_root.values[idx]\n",
        "        label3 = self.label.consonant_diacritic.values[idx]\n",
        "        image = self.data[idx, :].reshape(64,64).astype(np.float)\n",
        "        if self.transform:\n",
        "            augment = self.aug(image =image)\n",
        "            image = augment['image']\n",
        "            #image = augment['image'].astype(np.float32)\n",
        "            #cutout = Cutout(32,0.2,True,1)\n",
        "            #image = cutout(image)\n",
        "\n",
        "      \n",
        "        #norm = Normalize([0.0692],[0.2051])\n",
        "        #image = norm(image)\n",
        "        return image,label1,label2,label3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3TzvTSXB9y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Selayer(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes):\n",
        "        super(Selayer, self).__init__()\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv1 = nn.Conv2d(inplanes, int(inplanes / 16), kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(int(inplanes / 16), inplanes, kernel_size=1, stride=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.global_avgpool(x)\n",
        "\n",
        "        out = self.conv1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        return x * out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, cardinality, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes * 2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(planes * 2, planes * 2, kernel_size=3, stride=stride,\n",
        "                               padding=1, groups=cardinality, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes * 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(planes * 2, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "\n",
        "        self.selayer = Selayer(planes * 4)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out = self.selayer(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SeResNeXt(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, cardinality=32, num_classes=1000):\n",
        "        super(SeResNeXt, self).__init__()\n",
        "        self.cardinality = cardinality\n",
        "        self.inplanes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, self.cardinality, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, self.cardinality))\n",
        "                             \n",
        "        # vowel_diacritic\n",
        "        self.fc1 = nn.Linear(2048,11)\n",
        "        # grapheme_root\n",
        "        self.fc2 = nn.Linear(2048,168)\n",
        "        # consonant_diacritic\n",
        "        self.fc3 = nn.Linear(2048,7)\n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x1 = self.fc1(x)\n",
        "        x2 = self.fc2(x)\n",
        "        x3 = self.fc3(x)\n",
        "        \n",
        "        return x1,x2,x3\n",
        "\n",
        "\n",
        "def se_resnext50(**kwargs):\n",
        "    \"\"\"Constructs a SeResNeXt-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = SeResNeXt(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnext101(**kwargs):\n",
        "    \"\"\"Constructs a SeResNeXt-101 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = SeResNeXt(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def se_resnext152(**kwargs):\n",
        "    \"\"\"Constructs a SeResNeXt-152 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = SeResNeXt(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOyLckBQct15",
        "colab_type": "text"
      },
      "source": [
        "**wideresnet50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-QaWdKXcxoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    __constants__ = ['downsample']\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # vowel_diacritic\n",
        "        self.fc1 = nn.Linear(512 * block.expansion,11)\n",
        "        # grapheme_root\n",
        "        self.fc2 = nn.Linear(512 * block.expansion,168)\n",
        "        # consonant_diacritic\n",
        "        self.fc3 = nn.Linear(512 * block.expansion,7)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x1 = self.fc1(x)\n",
        "        x2 = self.fc2(x)\n",
        "        x3 = self.fc3(x)\n",
        "        return x1,x2,x3\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICccS81Jc2En",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
        "                   pretrained, progress, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTaHIaxRB-DY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTNf_KLk5EJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional  as F\n",
        "def criterion(input, target, size_average=True):\n",
        "    \"\"\"Categorical cross-entropy with logits input and one-hot target\"\"\"\n",
        "    l = -(target * torch.log(F.softmax(input, dim=1) + 1e-10)).sum(1)\n",
        "    if size_average:\n",
        "        l = l.mean()\n",
        "    else:\n",
        "        l = l.sum()\n",
        "    return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxHcaoOnCQvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = wide_resnet50_2().to(device)\n",
        "optimizer = optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,cycle_momentum = 0 ,base_lr=1e-3,max_lr=0.0008)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "batch_size=32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmPtOukBCQ63",
        "colab_type": "code",
        "outputId": "818d1384-bcc8-43d9-c5de-c1bc1e6c9ee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600,
          "referenced_widgets": [
            "3da49d2901c74f5bb7d9f6a02e51e005",
            "b4f673081a29415e8a352fe67abfee67",
            "f042dcd9d2a44793807c6e5c50ea20a3",
            "73316d3d2cd247d6879e43b7daca77ee",
            "232ac77d096d4a5e9af6ccb415c89d96",
            "ad4c050b4fcc46ed87749493fd8e2ff1",
            "8e0e3b5f8a904c28a4fa1f6df520e1be",
            "4a6f811aa8c540aeb24b1fda60393191",
            "2930fcf0d1e74c488cfe2c17cdd91768",
            "e51956b0c11845cbb0a5273403271284",
            "88e68bb064e04b84a471e811545c6403",
            "969d27fd32fb452c83b864fcb7d00429",
            "643fa59fb2994756adee4de0e13304a3",
            "d777e888339949b086c43ac254be0869",
            "15423d88752d4c14901f8750957d0112",
            "a16361ea565f4ca19a93861ce020eba0"
          ]
        }
      },
      "source": [
        "%%time\n",
        "epochs = 500\n",
        "model.train()\n",
        "losses = []\n",
        "accs = []\n",
        "for epoch in range(epochs):\n",
        "    reduced_index =train.groupby(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']).apply(lambda x: x.sample(5)).image_id.values\n",
        "    reduced_train = train.loc[train.image_id.isin(reduced_index)]\n",
        "    reduced_data = data_full.loc[data_full.image_id.isin(reduced_index)]\n",
        "    train_image = GraphemeDataset(reduced_data,reduced_train)\n",
        "    train_loader = torch.utils.data.DataLoader(train_image,batch_size=batch_size,shuffle=True)\n",
        "    \n",
        "    print('epochs {}/{} '.format(epoch+1,epochs))\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    for idx, (inputs,labels1,labels2,labels3) in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
        "        inputs = inputs.to(device)\n",
        "        labels1 = labels1.to(device)\n",
        "        labels2 = labels2.to(device)\n",
        "        labels3 = labels3.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs1,outputs2,outputs3 = model(inputs.unsqueeze(1).float())\n",
        "        loss1 = criterion(outputs1,labels1)\n",
        "        loss2 = criterion(outputs2,labels2)\n",
        "        loss3 = criterion(outputs3,labels3)\n",
        "        running_loss += loss1+loss2+loss3\n",
        "        running_acc += (outputs1.argmax(1)==labels1).float().mean()\n",
        "        running_acc += (outputs2.argmax(1)==labels2).float().mean()\n",
        "        running_acc += (outputs3.argmax(1)==labels3).float().mean()\n",
        "        (loss1+loss2+loss3).backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "    losses.append(running_loss/len(train_loader))\n",
        "    accs.append(running_acc/(len(train_loader)*3))\n",
        "    print('acc : {:.4f}%'.format(running_acc/(len(train_loader)*3)))\n",
        "    print('loss : {:.4f}'.format(running_loss/len(train_loader)))\n",
        "    if epoch > 300:\n",
        "      #torch.save(model.state_dict(), f'wide_resnet50_2_{epoch}.pth')\n",
        "      torch.save(model.state_dict(), f'/content/drive/My Drive/bengaliai/wide_resnet50_2__{epoch}.pth')\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs 1/500 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3da49d2901c74f5bb7d9f6a02e51e005",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=202), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "acc : 0.2745%\n",
            "loss : 8.8023\n",
            "epochs 2/500 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2930fcf0d1e74c488cfe2c17cdd91768",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=202), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-346bc24b9b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epochs = 500\\nmodel.train()\\nlosses = []\\naccs = []\\nfor epoch in range(epochs):\\n    reduced_index =train.groupby(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']).apply(lambda x: x.sample(5)).image_id.values\\n    reduced_train = train.loc[train.image_id.isin(reduced_index)]\\n    reduced_data = data_full.loc[data_full.image_id.isin(reduced_index)]\\n    train_image = GraphemeDataset(reduced_data,reduced_train)\\n    train_loader = torch.utils.data.DataLoader(train_image,batch_size=batch_size,shuffle=True)\\n    \\n    print('epochs {}/{} '.format(epoch+1,epochs))\\n    running_loss = 0.0\\n    running_acc = 0.0\\n    for idx, (inputs,labels1,labels2,labels3) in tqdm(enumerate(train_loader),total=len(train_loader)):\\n        inputs = inputs.to(device)\\n        labels1 = labels1.to(device)\\n        labels2 = labels2.to(device)\\n        labels3 = labels3.to(device)\\n        \\n        optimizer.zero_grad()\\n        outputs1,outputs2,outputs3 = model(inputs.unsqueeze(1).float())\\n        loss1 = criterion(outputs1,labels1)\\n        loss2 = criterion(outputs2,labels2)\\n        loss3 = criterion(outputs3,labels3)\\n        running_loss += loss1+loss2+loss3\\n        running_acc += (outputs1.argmax(1)==labels1).float().mean()\\n        running_acc += (outputs2.argmax(1)==labels2).float().mean()\\n        run...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-1812d1056934>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0maugment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m#image = augment['image'].astype(np.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, **data)\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdual_start_end\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdual_start_end\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, **data)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms_ps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                     )\n\u001b[1;32m     86\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36mapply_with_params\u001b[0;34m(self, params, force_apply, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mtarget_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_target_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mtarget_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dependence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtarget_dependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/transforms.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, img, gauss, **params)\u001b[0m\n\u001b[1;32m   2512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgauss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgauss_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgauss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgauss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_params_dependent_on_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py\u001b[0m in \u001b[0;36mwrapped_function\u001b[0;34m(img, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmaxval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_VALUES_BY_DTYPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(img, dtype, maxval)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m     \"\"\"\n\u001b[0;32m-> 2037\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         return _clip_dep_invoke_with_casting(\n\u001b[0;32m--> 132\u001b[0;31m             um.clip, a, min, max, out=out, casting=casting, **kwargs)\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip_dep_invoke_with_casting\u001b[0;34m(ufunc, out, casting, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# try to deal with broken casting rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_UFuncOutputCastingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Numpy 1.17.0, 2019-02-24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr4VMTjfCQ92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_full"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuJT21uJCRDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTjpwyB4CRH2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWFkopuMCRA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}